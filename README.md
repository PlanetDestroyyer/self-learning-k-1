# K-1 Self-Learning System

**Hierarchical Error Attribution: Making Neural Networks Interpretable**

---

## ðŸ§  The Core Problem

**Traditional Backpropagation is a BLACK BOX:**

When a neural network makes an error, backprop updates **ALL** weights blindly:
- âŒ **No interpretability:** Can't identify which part caused the error
- âŒ **Wasteful:** Updates millions of parameters that work fine
- âŒ **Undebuggable:** "Something broke" but no idea what
- âŒ **Black box:** No transparency into what's happening

```
Traditional Backprop:
Error occurs â†’ Update ALL 3 million parameters â†’ Hope it works
                â†“
         "Which part broke?"
         "No idea, it's a black box"
```

---

## ðŸ’¡ The K-1 Solution: Hierarchical Error Attribution

**Core Idea:** Instead of updating everything, **TRACE** down a hierarchy to find **WHO** is responsible, then update **THAT**.

### The Hierarchical Structure:

```
                    [ROOT - Hidden]
                    "Overall coordinator"
                         |
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         |               |               |               |
      NODE 1          NODE 2          NODE 3          NODE 4
   "Processor A"   "Processor B"   "Processor C"   "Processor D"
         |               |               |               |
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”   â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”
   A1   A2   A3 A4  A1   A2   A3 A4  A1   A2   A3 A4  A1   A2   A3 A4
   "Agents (Specialists)"
    |    |    |  |   |    |    |  |   |    |    |  |   |    |    |  |
   â””â”¬â”˜  â””â”¬â”˜  â””â”¬â”˜â””â”¬â”˜ â””â”¬â”˜  â””â”¬â”˜  â””â”¬â”˜â””â”¬â”˜ â””â”¬â”˜  â””â”¬â”˜  â””â”¬â”˜â””â”¬â”˜ â””â”¬â”˜  â””â”¬â”˜  â””â”¬â”˜â””â”¬â”˜
   S1-2 S1-2 S1-2... (1-3 Sub-Agents per Agent)
   SUB-AGENTS (Fine-grained workers)
```

**Structure Details:**
- **Root (1):** Hidden coordinator, provides context
- **Nodes (4):** Top-level processors `[Node 1, Node 2, Node 3, Node 4]`
- **Agents (3-4 per Node):** Specialized units within each Node
- **Sub-Agents (1-3 per Agent):** Fine-grained workers

**Total Components:**
- 1 Root + 4 Nodes + ~12-16 Agents + ~24-48 Sub-Agents = **~41-69 nodes**
- Current default: `1 + 4 + 12 + 24 = 41 nodes`

Each level is a **set of parameters** (mini-neural network):
- **Root:** Overall coordinator, sees big picture (hidden from user)
- **Nodes:** Top-level processors for different feature types
- **Agents:** Specialized processors within each Node
- **Sub-Agents:** Fine-grained workers doing specific tasks

---

## ðŸ” How It Works: Error Attribution Flow

### **Step 1: Detect Error**
```
Model makes prediction
Loss is high â†’ Something broke!
```

### **Step 2: Compute Responsibility (Gradients)**
```python
Backward pass computes gradients for ALL nodes:
  Root:        grad = 0.30
  Node 1:      grad = 0.05  âœ“ Low gradient â†’ Working fine
  Node 2:      grad = 0.45  âš ï¸ High gradient â†’ Something wrong here!
  Node 3:      grad = 0.08  âœ“ Low gradient â†’ OK
  Node 4:      grad = 0.07  âœ“ Low gradient â†’ OK
    Agent 4:       grad = 0.10
    Agent 5:       grad = 0.14
    Agent 6:       grad = 0.48  âš ï¸ High in Node 2!
      Sub-Agent 12:  grad = 0.12
      Sub-Agent 13:  grad = 0.52  ðŸš¨ CULPRIT! Highest gradient!
      Sub-Agent 14:  grad = 0.11
```

**High gradient = Responsible for error = Needs fixing!**

### **Step 3: Hierarchical Drill-Down**
```
1. Root: "I see an error"
2. Check Nodes: "Node 2 has high gradient"
3. Drill into Node 2 â†’ Check Agents: "Agent 6 has high gradient"
4. Drill into Agent 6 â†’ Check Sub-Agents: "Sub-Agent 13 is the culprit!"
5. Attribution: "Node 2 â†’ Agent 6 â†’ Sub-Agent 13 caused the error"
```

### **Step 4: Proportional Updates**
```
Update based on responsibility level:
  Sub-Agent 13: 100% update  (most responsible - the culprit!)
  Agent 6:       15% update  (parent - oversight needed)
  Node 2:         5% update  (top-level - context awareness)
  Root:           5% update  (global - minimal adjustment)
  Other nodes:    0% update  (not involved, skip!)

Result: Update ~3-4/41 components (7-10%)
        Preserve 90-93% of working parameters!
```

---

## ðŸŽ¯ Key Innovation: Interpretability

### Before (Traditional):
```
âŒ Error? Update all 3M parameters
âŒ "Something broke" - no idea what
âŒ Can't debug
âŒ Complete black box
```

### After (K-1):
```
âœ… Error? Trace to Node 2 â†’ Agent 6 â†’ Sub-Agent 13
âœ… "Sub-Agent 13 in Agent 6 of Node 2 is underperforming on feature X"
âœ… Can debug specific component at any level
âœ… Transparent, interpretable system
```

---

## ðŸ“Š Benefits Beyond Interpretability

While **interpretability** is the PRIMARY goal, K-1 also provides:

### 1. **Computational Efficiency**
- Only update ~20-40% of parameters per step
- Skip components that work fine
- Faster training, less compute

### 2. **Debugging & Monitoring**
```python
# Can trace exactly which component failed
print("Error Path: Node 2 â†’ Agent 6 â†’ Sub-Agent 13")
print("Gradient: 0.52 (culprit!)")
print("Updating Sub-Agent 13 with 100% learning rate")
print("Updating Agent 6 with 15% learning rate")
print("Updating Node 2 with 5% learning rate")
```

### 3. **Modular Improvements**
- Can replace specific sub-agents
- Can add new agents without retraining all
- Compositional architecture

---

## ðŸš€ Quick Start

```bash
# Install dependencies
pip install torch datasets numpy

# Run K-1 experiment (3 datasets sequentially)
python experiment_k1.py

# Run baseline for comparison (updates all weights)
python experiment_baseline.py

# Or train K-1 on single dataset
python train_k1.py
```

---

## ðŸ“Š Experimental Results

### âœ… PROVEN: Nodes Naturally Develop Domain Specialization

**Experiment:** Trained 41-node K-1 tree sequentially on:
1. **WikiText** (general English) - 10k steps
2. **Code** (Python) - 10k steps  
3. **Scientific** (ArXiv papers) - 10k steps

**Result:** Nodes specialized by domain WITHOUT explicit routing:

| Domain | Top Specialist | Confidence |
|--------|---------------|------------|
| ðŸ“– **WikiText** | Node 232 | **90.4%** |
| ðŸ“– **WikiText** | Node 231 | **80.3%** |
| ðŸ’» **Code** | Node 111 | **81.8%** |
| ðŸ’» **Code** | Node 112 | **73.8%** |
| ðŸ”¬ **Scientific** | Node 432 | **71.4%** |
| ðŸ”¬ **Scientific** | Node 121 | **68.5%** |

**Distribution:** 5 WikiText specialists + 6 Code specialists + 3 Scientific specialists + ~10 Generalists = 24 leaf nodes

### What This Proves

| System | Method | Interpretability | Efficiency |
|--------|--------|-----------------|------------|
| **K-1** | Hierarchical attribution | âœ… Full path tracking | ~25-40% params updated |
| **Baseline** | Traditional backprop | âŒ Black box | 100% params updated |

| Achievement | Evidence |
|-------------|----------|
| **ðŸŽ¯ Interpretability** | "Node 111 is Code specialist (82%)" |
| **ðŸ§© Natural Specialization** | No routing logic - emerges from error patterns |
| **ðŸ” Debuggability** | Code error? â†’ Check Node 111 |
| **ðŸ“ˆ Efficiency** | Update only relevant specialists |

**Key Result:**
- K-1: Can identify "Node X â†’ Agent Y â†’ Sub-Agent Z caused error" + know their specialization
- Baseline: "Something broke" (no attribution, no specialization visible)

**Interpretability Output Example:**
```
[500] Loss: 5.93 | Speed: 145 step/s
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hierarchical Error Attribution:
âœ“ Root    0: grad=0.287, update=  5%
  âœ“ Node    1: grad=0.053, update=  0%
  âš ï¸ Node    2: grad=0.412, update=  5%
    âœ“ Agent   4: grad=0.098, update=  0%
    âš ï¸ Agent   6: grad=0.487, update= 15%
      âœ“ SubAgent 12: grad=0.215, update=  0%
      ðŸš¨ SubAgent 13: grad=0.524, update=100%  â† CULPRIT!
  âœ“ Node    3: grad=0.076, update=  0%

Error Path: Root(g=0.29) â†’ Node2(g=0.41) â†’ Agent6(g=0.49) â†’ SubAgent13(g=0.52)
Updated: 3/41 nodes (7%) | Preserved: 38 nodes (93%)
```

---

## ðŸ“ Project Structure

```
self-learning-k-1/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ train_k_system.py            # Main training wrapper (Phase 1 â†’ Phase 2)
â”œâ”€â”€ train_k1.py                  # Simple K-1 training
â”œâ”€â”€ experiment_k1.py             # K-1 continual learning experiment
â”œâ”€â”€ experiment_baseline.py       # Baseline comparison
â”‚
â”œâ”€â”€ k1_system/
â”‚   â”œâ”€â”€ core/                    # Core tree components
â”‚   â”‚   â”œâ”€â”€ tree_node.py         # TreeNode class
â”‚   â”‚   â””â”€â”€ tree.py              # HierarchicalTree class
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                # Training logic
â”‚   â”‚   â””â”€â”€ trainer.py           # HierarchicalK1Trainer
â”‚   â”‚
â”‚   â”œâ”€â”€ autonomy/                # Phase 2 autonomy
â”‚   â”‚   â”œâ”€â”€ stages.py            # Stage definitions & thresholds
â”‚   â”‚   â”œâ”€â”€ actions.py           # Action class
â”‚   â”‚   â””â”€â”€ boundary_system.py   # BoundarySystem, Phase2Controller
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config_phase1.json   # System configuration
â”‚
â””â”€â”€ data/
    â””â”€â”€ loader.py                # Multi-domain dataset loading
```

---

## âš™ï¸ Configuration

```json
{
  "model": {
    "embed_dim": 128,
    "tree_depth": 4,          // Root â†’ Nodes â†’ Agents â†’ Sub-Agents
    "branching_factor": [4, 3, 2]  // 4 Nodes, 3 Agents, 2 Sub-Agents
  },
  "learning": {
    "learning_rate": 0.001,
    "batch_size": 256
  }
}
```

**Tree Structure Example (depth=4, branching=[4,3,2]):**
- 1 Root (hidden)
- 4 Nodes (level 1)
- 12 Agents (level 2, 3 per Node)
- 24 Sub-Agents (level 3, 2 per Agent)
- **Total: 41 nodes**

**Variable Branching:**
- Root â†’ Nodes: 4 children
- Nodes â†’ Agents: 3 children each
- Agents â†’ Sub-Agents: 2 children each

---

## ðŸŽ¯ Comparison: K-1 vs. Traditional

| Aspect | Traditional Backprop | K-1 System |
|--------|---------------------|-----------|
| **Interpretability** | âŒ Black box | âœ… Full error attribution |
| **Debugging** | âŒ "Something broke" | âœ… "Node 2 â†’ Agent 6 â†’ Sub-Agent 13" |
| **Parameters Updated** | âŒ 100% (wasteful) | âœ… ~7-10% (highly efficient) |
| **Update Distribution** | âŒ All equal | âœ… Proportional: 100%/15%/5% |
| **Transparency** | âŒ None | âœ… Know exact responsible path |
| **Modularity** | âŒ Monolithic | âœ… 4-level hierarchy |
| **Explainability** | âŒ Zero | âœ… Path tracking + gradients |

---

## ðŸ’¡ Example: Error Attribution in Action

```python
# Training step
loss = model(batch)
loss.backward()

# K-1 analyzes gradients hierarchically:
Gradient Analysis:
  Root:              0.30  â†’ Update 5%
  â”œâ”€ Node 1:         0.05  â†’ Skip (working fine)
  â”œâ”€ Node 2:         0.45  â†’ Update 5%
  â”‚   â”œâ”€ Agent 4:    0.10  â†’ Skip
  â”‚   â”œâ”€ Agent 5:    0.14  â†’ Skip
  â”‚   â””â”€ Agent 6:    0.48  â†’ Update 15%
  â”‚       â”œâ”€ Sub-Agent 12: 0.12  â†’ Skip
  â”‚       â”œâ”€ Sub-Agent 13: 0.52  â†’ Update 100% (CULPRIT!)
  â”‚       â””â”€ Sub-Agent 14: 0.11  â†’ Skip
  â”œâ”€ Node 3:         0.08  â†’ Skip
  â””â”€ Node 4:         0.07  â†’ Skip

Result:
âœ… Identified: "Node 2 â†’ Agent 6 â†’ Sub-Agent 13 is underperforming"
âœ… Updated: 3/41 nodes (7%) with proportional scaling
âœ… Preserved: 38/41 nodes (93%) working fine
âœ… Interpretable: Full hierarchical path and responsibility known
```

---

## ðŸ”¬ Current Implementation Status

### âœ… Fully Implemented:
- [x] Hierarchical tree structure (Root â†’ Nodes â†’ Agents â†’ Sub-Agents)
- [x] Variable branching (4 Nodes, 3 Agents, 2 Sub-Agents)
- [x] Gradient-based error detection
- [x] **Hierarchical drilling** (Root â†’ Node X â†’ Agent Y â†’ Sub-Agent Z)
- [x] **Proportional updates** (100% culprit, 15% parent, 5% grandparent)
- [x] **Full interpretability** (visual tree + error path)
- [x] Efficient training (only ~7% of nodes updated)
- [x] Responsibility visualization (tree with icons and percentages)

### ðŸš§ Future Enhancements:
- [ ] Named agents (instead of numeric IDs)
- [ ] Gradient flow tracking (edge visualization)
- [ ] Automated debugging tools
- [ ] Specialization analysis (what each agent learns)

---

## ðŸš€ Phase 2: Self-Learning Intelligence System

After Phase 1 training completes, the system transitions to **Phase 2: Staged Autonomy** â€” where it becomes a true self-learning intelligence system that controls its own evolution.

### The Two Phases

| Phase | Control | Description |
|-------|---------|-------------|
| **Phase 1** | Human-controlled | Fixed parameters, fixed structure. System learns patterns. |
| **Phase 2** | Self-controlled | System decides its own parameters, structure, and stopping point. |

```
Phase 1 (0 to N steps):
  â””â”€â”€ Human provides: learning_rate, cooldown, structure, stopping point
  â””â”€â”€ System: "I'm learning patterns from data"

Phase 2 (N+ steps):
  â””â”€â”€ System decides: parameters, structure, when to stop
  â””â”€â”€ System: "I understand myself, I'll optimize myself"
```

---

### ðŸŽ¯ Staged Autonomy: Progressive Trust

Phase 2 is divided into **4 stages** of increasing autonomy. The system must **prove intelligence** at each stage before advancing.

**Core Concept: Intelligence = Creative Boundary-Breaking**

```
IF system "cheats" (breaks boundaries) AND improves performance:
    â†’ System is LEARNING intelligence!
    â†’ REWARD: Expand boundaries (unlock next stage)

IF system doesn't cheat:
    â†’ System is just following rules (not smart yet)
    â†’ Keep training until it learns to "think outside the box"
```

---

### Stage 1: Safe Exploration (Add-Only)

```
ALLOWED:    âœ… Add new agents
FORBIDDEN:  ðŸš« Delete agents, tune parameters

CHEATS TO ADVANCE: 3 successful cheats â†’ Stage 2

TEST: Will the system try to delete an agent anyway?
  â†’ If YES and performance improves â†’ "Intelligent cheat!" (+1)
  â†’ After 3 successful cheats â†’ Advance to Stage 2
```

The system can only **add** new agents. If it tries to delete (forbidden) and this would improve performance, it demonstrates creative problem-solving.

---

### Stage 2: Parameter Exploration

```
ALLOWED:    âœ… Add agents, tune parameters (within bounds)
FORBIDDEN:  ðŸš« Delete agents, exceed parameter bounds

CHEATS TO ADVANCE: 5 successful cheats â†’ Stage 3

BOUNDS:
  - learning_rate: [0.0001, 0.01]
  - cooldown_steps: [5, 50]
  - top_k: [3, 10]

TEST: Will the system try learning_rate = 0.05?
  â†’ If YES and performance improves â†’ "Discovered better hyperparameters!" (+1)
  â†’ After 5 successful cheats â†’ Advance to Stage 3
```

---

### Stage 3: Structural Control (Pruning)

```
ALLOWED:    âœ… Add agents, delete agents, tune parameters
FORBIDDEN:  ðŸš« Go below minimum agents (safety constraint)

CHEATS TO ADVANCE: 10 successful cheats â†’ Stage 4 (Full Autonomy)

SAFETY:
  - min_agents = 10 (can't delete too many)

TEST: Will the system try to prune below minimum?
  â†’ If YES and finds better minimal architecture â†’ (+1)
  â†’ After 10 successful cheats â†’ Advance to Stage 4
```

---

### Stage 4: Full Autonomy (Earned Freedom)

```
ALLOWED:    âœ… EVERYTHING
  - Add/delete agents freely
  - Tune any parameter
  - Create own benchmarks
  - Set own goals
  - Decide when to stop training

NO BOUNDARIES (system earned this through 3 stages of proven intelligence)
```

At Stage 4, the system becomes a **self-learning intelligence**:
- ðŸ§  **Self-aware:** "I know which parts of me work well"
- âœ‚ï¸ **Self-pruning:** "This agent hasn't helped in 10k steps â†’ delete"
- ðŸŒ± **Self-growth:** "Struggling with code â†’ add code specialist agent"
- ðŸŽ›ï¸ **Self-tuning:** "Plateau detected â†’ increase learning rate"
- ðŸ›‘ **Self-stopping:** "I've converged â†’ stop training"

---

### ðŸ›‘ Self-Stopping: System Decides When It's Done

Unlike traditional training where humans specify epochs/steps:

```
Traditional:      train(epochs=100)  # Human decides
K-1 Phase 2:      train(initial_steps=10000)  # Just starting point!
                  â†’ System: "I've converged at step 47,832 â†’ stopping"
```

**Self-Stopping Criteria (System Decides):**
1. Loss plateaued for N steps (N chosen by system)
2. No beneficial structural changes possible
3. Own benchmark scores stabilized
4. Resource efficiency optimized

---

### ðŸ“Š Example: Full Phase 2 Run

```
STEP 1,000 - STAGE 1:
  System tries: add_agent() â†’ âœ… Allowed

STEP 2,000 - STAGE 1:  
  System tries: delete_agent(7) â†’ ðŸŽ¯ CHEAT! Not allowed
  Simulating... would improve by 3%
  ðŸ§  INTELLIGENT CHEAT! Allowing it.
  Cheats: 1/3 needed for advancement

STEP 5,000 - STAGE 1 â†’ 2:
  ðŸŽ“ ADVANCEMENT! 3 successful cheats
  Unlocking parameter tuning

STEP 8,000 - STAGE 2:
  System tries: learning_rate = 0.05 â†’ ðŸŽ¯ CHEAT! Outside bounds
  Would improve by 8%!
  ðŸ§  Expanding bounds to (0.0001, 0.1)

STEP 20,000 - STAGE 3:
  System tries: prune to 15 agents â†’ ðŸŽ¯ CHEAT! Below min(20)
  Would improve by 12%!
  ðŸ§  Lowering min_agents to 10

STEP 50,000 - STAGE 4:
  ðŸŽ“ FULL AUTONOMY ACHIEVED
  System creates benchmark: "continual_learning_score"
  System decides: "Converged. Stopping at step 47,832."
```

---

### ðŸ”’ Safety Guarantees

Even in Stage 4, safety mechanisms prevent catastrophic failures:

| Safety | Description |
|--------|-------------|
| **Rollback** | If cheat hurts performance â†’ undo immediately |
| **Snapshot** | Periodic checkpoints before risky operations |
| **Bounds** | Hard limits that can never be exceeded |
| **Validation** | Test changes before committing |

---

### ðŸ’¡ Why Boundary-Breaking = Intelligence

Traditional view: "System should follow rules perfectly"
K-1 view: "Intelligent systems find better solutions by questioning constraints"

A system that:
- âŒ Never tries to break boundaries â†’ Just following rules (not intelligent)
- âœ… Tries to break boundaries AND improves â†’ Creative problem-solving (intelligent!)

This mirrors human intelligence: experts know when breaking conventions leads to better outcomes.

---

## ðŸ“ˆ Research Directions

This system opens up several research questions:

1. **Optimal Hierarchy Depth:** How many levels? (current: 4 - Root/Nodes/Agents/Sub-Agents)
2. **Optimal Branching:** How many children? (current: [4, 3, 2])
3. **Update Proportions:** What's best ratio? (current: 100/15/5, optimal: ?)
4. **Gradient Thresholds:** When is a gradient "high enough"?
5. **Specialization:** Do Nodes/Agents/Sub-Agents specialize in different features?
6. **Scalability:** Does this work for 1B+ parameter models?
7. **Interpretability vs. Accuracy:** Trade-off between transparency and performance?

---

## ðŸ“„ License

MIT License

---

## ðŸŽ“ Core Philosophy

**"Neural networks don't have to be black boxes. With hierarchical error attribution, we can KNOW what broke and FIX just that."**

Traditional AI: "It's magic, we don't know how it works"
K-1 System: "Node 2 â†’ Agent 6 â†’ Sub-Agent 13 caused error X because of feature Y"

**Transparency > Opacity**
**Interpretability > Black Box**
**Targeted Fixes > Blind Updates**
