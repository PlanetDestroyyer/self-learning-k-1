# K-1 Self-Learning System

**Hierarchical Path-Based Learning: Update Only What's Broken**

---

## ğŸ§  The Core Idea

**Traditional Backprop Problem:** Updates ALL weights every step, even those working perfectly. This causes:
- Catastrophic forgetting (old knowledge erased by new)
- Black box (no idea which parts learned what)
- Wasted compute (updating weights that don't need it)

**K-1 Solution:** Build a hierarchical tree of nodes. Only update the PATH responsible for errors.

```
                    ROOT (Manager)
                         |
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         |               |               |
      Node 1          Node 2          Node 3
         |               |               |
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
   L1   L2   L3    L4   L5   L6   L7   L8   L9
```

### How It Works:

1. **Forward:** Data flows through tree
2. **Loss:** Computed at output  
3. **Backward:** Compute gradients for ALL nodes
4. **Analyze:** Which nodes have HIGH gradients? (causing errors)
5. **Update:** Only high-gradient nodes, skip the rest

```
Gradient Analysis:
  Root:    grad = 0.30  âœ… Update
  Node 1:  grad = 0.05  âŒ Skip (fine)
  Node 2:  grad = 0.45  âœ… Update (problem!)
  Node 3:  grad = 0.08  âŒ Skip
    â””â”€â”€ L5: grad = 0.52  âœ… Update (culprit!)

Result: Update 3/13 nodes (23%)
        Preserve 77% of weights!
```

---

## ğŸš€ Quick Start

```bash
# Install
pip install torch datasets numpy

# Run K-1 experiment (3 datasets)
python experiment_k1.py

# Run baseline for comparison
python experiment_baseline.py

# Or just train K-1 on WikiText
python train_k1.py
```

---

## ğŸ“Š Experiments

### Continual Learning Test

Both experiments train on 3 datasets sequentially:
1. **WikiText-2** (general English)
2. **Code** (Python)
3. **Scientific** (research papers)

After each dataset, we evaluate on ALL previous datasets to measure **forgetting**.

| Script | Method | Expected Forgetting |
|--------|--------|-------------------|
| `experiment_k1.py` | K-1 (sparse path updates) | Low (~10-20%) |
| `experiment_baseline.py` | Traditional (update ALL) | High (~50%+) |

---

## ğŸ“ Project Structure

```
self-learning-k-1/
â”œâ”€â”€ train_k1.py              # Train K-1 on single dataset
â”œâ”€â”€ experiment_k1.py         # K-1 continual learning experiment
â”œâ”€â”€ experiment_baseline.py   # Baseline experiment for comparison
â”œâ”€â”€ k1_system/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ hierarchical_tree.py  # TreeNode + HierarchicalTree
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config_phase1.json
â””â”€â”€ data/
    â””â”€â”€ loader.py            # Dataset loading
```

---

## âš™ï¸ Configuration

```json
{
  "model": {
    "embed_dim": 128,
    "tree_depth": 3,          // Levels in tree
    "branching_factor": 3     // Children per node
  },
  "learning": {
    "top_k": 5,               // Update top 5 nodes
    "batch_size": 256
  }
}
```

---

## ğŸ¯ Key Benefits

| Feature | Traditional | K-1 |
|---------|-------------|-----|
| **Params Updated** | 100% | ~25-40% |
| **Forgetting** | High | Low |
| **Explainability** | None | Full path tracking |
| **Debugging** | Hard | "Node 2 â†’ L5 is broken" |

---

## ğŸ“„ License

MIT License
